{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36eae6af6a8b8707f14906e588a3ee64",
     "grade": false,
     "grade_id": "cell-a5d8c8596524d405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ELEC 400M / EECE 571M Assignment 1: Linear models for classification\n",
    "(This assignment is a modified version of an assignment used in ECE 421 at the University of Toronto and kindly made available to us by the instructor.)\n",
    "\n",
    "In this assignment, you will be using linear models discussed in the lectures to perform a binary classification task. You will compare the performances of linear classification and logistic regression using suitable training algorithms. The implementation will be done in python using functions from the NumPy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9a8b3c4101c3b8441a31e218145894f",
     "grade": false,
     "grade_id": "cell-6a4eff1ae721ca9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data Set\n",
    "We consider the dataset of images of letters contained in file notMNIST.npz. In particular, you will use a smaller dataset that only contains the images from two letter classes: “C” (the positive class) and “J” (the negative class). The images are of size 28 × 28 pixels. The figure below shows 20 randomly selected image samples for the letters “C” and “J”.\n",
    "\n",
    "![](sample_images.eps)\n",
    "\n",
    "You will apply the function `loadData` to generate the subset of images containing only letters “C” and “J”. This script organizes the total set of 3,745 images into maller subsets containing 3,500 training images, 100 validation images and 145 test images. Their use will be further specified in the problem descriptions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "734015703ccb840f57bb43468c573784",
     "grade": false,
     "grade_id": "cell-eef20adb07056077",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a07dc59b87c2b66db4da8655d72e3bf1",
     "grade": false,
     "grade_id": "cell-9d51b6e8a7c666a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    with np.load('notMNIST.npz') as data:\n",
    "        Data, Target = data['images'], data['labels']\n",
    "        posClass = 2\n",
    "        negClass = 9\n",
    "        dataIndx = (Target==posClass) + (Target==negClass)\n",
    "        Data = Data[dataIndx]/255.0\n",
    "        Target = Target[dataIndx].reshape(-1, 1)\n",
    "        Target[Target==posClass] = 1\n",
    "        Target[Target==negClass] = 0\n",
    "        np.random.seed(1)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data, Target = Data[randIndx], Target[randIndx]\n",
    "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
    "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
    "        testData, testTarget = Data[3600:], Target[3600:]\n",
    "       \n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f087958f088ae0145d1e68b806545c8",
     "grade": false,
     "grade_id": "cell-5b5bf0983e23b493",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Classification\n",
    "\n",
    "The first classifier is the linear classifier \n",
    "$$\\hat{y}=\\mathrm{sign}\\left(\\sum_{i=0}^dw_ix_i\\right)\\,$$\n",
    "where $x_0=1$ so that $b=w_0x_0$ is the bias term and $x_1,\\ldots, x_d$ are the input features.\n",
    "The loss function for an input-output pair $(\\underline{x}_n,y_n)$ and given model parameters $\\underline{w}$ is \n",
    "$$L_n(\\underline{w})= \\mathbf{1}\\{\\hat{y}_n\\neq y_n\\}\\;.$$\n",
    "The total loss for $N$ samples is \n",
    "$$L(\\underline{w})= \\frac{1}{N}\\sum\\limits_{n=1}^N\\mathbf{1}\\{\\hat{y}_n\\neq y_n\\}\\;.$$\n",
    "\n",
    "\n",
    "\n",
    "### Notes on Classification\n",
    "* The classification should be based on the $d=28\\times 28=784$ intensity values in an image. This means that you need to flatten the 2D images to 1D input vectors $\\underline{x}$ of length 784.\n",
    "* The outputs $\\hat{y}$ of the perceptron model are from $\\{-1,+1\\}$, while the target variable from the data set is from $\\{0,1\\}$. You need to make adjustements to account for this difference, which can include adjusting the data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6af4e6b0785c4453b46bf6443a37c31b",
     "grade": false,
     "grade_id": "cell-39aa5707539633da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss Function [2 points]\n",
    "\n",
    "Implement a function to compute the classification loss as defined above. The function has three input arguments: the weight vector, the feature vectors, and the labels. It returns the total loss associated with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "818a490ac40937e4f87242012417534a",
     "grade": true,
     "grade_id": "cell-ae171a05a9de28c1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ErrorRate (w, x, y):\n",
    "    # Find y_cap\n",
    "    y_cap = ((np.matmul(x,w))>0).astype(int)\n",
    "    # Note: y_cap needs to be changed to [-1,1]\n",
    "    y_cap[y_cap<1]=-1\n",
    "    # Calculate Total Loss for N Samples\n",
    "    loss = (((y_cap!=y).astype(float)).sum())/y.size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9b449fbdd732dea70062ea3abde3e73",
     "grade": false,
     "grade_id": "cell-34e3d06afbbedc4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Perceptron Learning Algorithm [10 points]\n",
    "\n",
    "Implement a function for the perceptron learning algorithm (PLA) which accepts four arguments: an inital weight vector, the data, the labels, and the maximal number of iterations it executes. It is thus a version of the PLA is assured to terminate. The function returns the updated weight vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfbe75572c9845886002b30c30286e90",
     "grade": true,
     "grade_id": "cell-8968d65ea48b9d30",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def PLA(w, x, y, maxIter):\n",
    "    # Start PLA Loop\n",
    "    while((maxIter>0)and(ErrorRate(w,x,y)>0)):\n",
    "        \n",
    "        # First Identify Misclassified Samples\n",
    "        y_cap = ((np.matmul(x,w))>0).astype(int)\n",
    "        # Note: y_cap needs to be changed to [-1,1]\n",
    "        y_cap[y_cap<1]=-1\n",
    "        r = (y_cap!=y).astype(int)\n",
    "        \n",
    "        # Pick Random Sample To Use for Weight Update\n",
    "        \n",
    "        # Create an Array to Store Indices of\n",
    "        # misclassified samples\n",
    "        s = np.zeros(r.sum())\n",
    "        \n",
    "        sInd = 0 #Index for s\n",
    "        i = r.shape[0] #Index for Inner Loop\n",
    "        while(i>0):\n",
    "            i-=1\n",
    "            if r[i][0]==1:\n",
    "                s[sInd]=i\n",
    "                sInd+=1\n",
    "        \n",
    "        # Pick Random Index\n",
    "        rInd = int(np.random.choice(s))\n",
    "        #Weight Update\n",
    "        tempx = x[rInd].reshape(x[rInd].shape[0],1)\n",
    "        w = np.add(w,(y[rInd][0]*(tempx)))\n",
    "        \n",
    "        maxIter-=1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27aa2ab61179ae69bfc9d3a3b4913d73",
     "grade": false,
     "grade_id": "cell-71e4ba41ec059bc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test the `PLA` function by training the classifier on the training data (`trainData`, `trainTarget`) with a maximum of 100 iterations, and measuring the cassification error using the testing data (`testData`, `testTarget`). Write the test script into the box below and let it print the classiciation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97993999c368017a94b44290429d704a",
     "grade": true,
     "grade_id": "cell-f6ac3ead6dede7dc",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Classification Error using the testing data is 0.027586 or 2.758621%\n"
     ]
    }
   ],
   "source": [
    "d = loadData()\n",
    "trainData = d[0].reshape(3500,-1)\n",
    "trainData = np.append(np.ones((trainData.shape[0],1)),trainData,axis=1)\n",
    "\n",
    "validData = d[1].reshape(100,-1)\n",
    "testData = d[2].reshape(145,-1)\n",
    "testData = np.append(np.ones((testData.shape[0],1)),testData,axis=1)\n",
    "\n",
    "trainTarget = d[3].astype(int)\n",
    "trainTarget[trainTarget<1]=-1\n",
    "validTarget = d[4].astype(int)\n",
    "validTarget[validTarget<1]=-1\n",
    "testTarget = d[5].astype(int)\n",
    "testTarget[testTarget<1]=-1\n",
    "\n",
    "# Training with Train Data\n",
    "wPLA = PLA(np.zeros((785,1)),trainData,trainTarget,100) #Add 1 to w for bias\n",
    "\n",
    "# Test with Test Data\n",
    "EClassTest = ErrorRate(wPLA,testData,testTarget)\n",
    "print(\"The Classification Error using the testing data is {:f} or {:f}%\".format(EClassTest,EClassTest*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d48a3c6c215801e25502e9d265edcd29",
     "grade": false,
     "grade_id": "cell-beeae3997a377968",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Pocket algorithm [14 points]\n",
    "\n",
    "Implement a function for the pocket algorithm which accepts three arguments: the data, the labels, and the number of iterations it executes. It should use the function `PLA` you developed above. It returns the updated weight vector.\n",
    "\n",
    "First, briefly describe how your pocket algorithm works, and how it calls the function `PLA` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f36bbd3bd478262c9b7a824869b20c5",
     "grade": true,
     "grade_id": "cell-a8758f5f83e66912",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The pocket function takes in input data (x), labels (y) and number of iterations (T) as arguments and returns weight vector $\\underline{w}$. The function has been designed to implement the following algorithm.\n",
    "\n",
    "1. Initialize weight vector, $\\underline{w}$ to all zeros and also set $\\underline{w}_{Pocket}$ to this.\n",
    "2. While T>0\n",
    "3. ---- Call the PLA function using $\\underline{w}$,x and y for **one** update to get $\\underline{w}_{new}$\n",
    "4. ---- Calculate $E_{New}$ which is the classification error $E_{in}\\left(\\underline{w}_{new}\\right)$\n",
    "5. ---- Calculate $E_{Pocket}$ which is the classification error $E_{in}\\left(\\underline{w}_{Pocket}\\right)$\n",
    "6. ---- If $E_{New}$ < $E_{Pocket}$ **:** Update $\\underline{w}_{Pocket}$ to $\\underline{w}_{new}$\n",
    "7. ---- Set $\\underline{w}$ to $\\underline{w}_{new}$ for the next iteration\n",
    "8. ---- Decrement T\n",
    "9. Return $\\underline{w}_{Pocket}$ at the end of T iterations\n",
    "\n",
    "The Pocket Algorithm continues to retain the \"best\" weight vector until there is an updated weight vector that reduces the classification error further. In case of data that is not linearly separable, PLA can keep varying between weight vectors and might not necessarily return the best. In such a scenario, running the pocket algorithm (over a large number of iterations) can at least yield the best set of weights that minimizes the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9641cc289d555169a3b843903f1b5452",
     "grade": true,
     "grade_id": "cell-6db776985174dd90",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Pocket Algorithm Implementation\n",
    "def pocket(x, y, T):\n",
    "    # Init Weight Vector\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "    wPocket = np.copy(w)\n",
    "    while(T>0):\n",
    "        wNew = PLA(w,x,y,1)\n",
    "        ENew = ErrorRate(wNew,x,y)\n",
    "        EPocket = ErrorRate(wPocket,x,y)\n",
    "        if ENew < EPocket:\n",
    "            wPocket = np.copy(wNew)\n",
    "        w = np.copy(wNew)\n",
    "        T-=1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5f59bdc0a619eaf1c7fbecc1850c6ca",
     "grade": false,
     "grade_id": "cell-d4700c0f8f5efa9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test the `pocket` function by training the classifier on the training data `(trainData, trainTarget)` with 100 iterations, and measuring the cassification error using the testing data `(testData, testTarget)`. Write the test script into the box below and let it print the classiciation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75dcbaf25c695d58723ac914b8f11732",
     "grade": true,
     "grade_id": "cell-87a6bd87fb24fb25",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Classification Error using the testing data is 0.034483 or 3.448276% (Pocket)\n"
     ]
    }
   ],
   "source": [
    "# Training with Pocket Algorithm Using Train Data\n",
    "wPock = pocket(trainData,trainTarget,100)\n",
    "\n",
    "# Testing with Test Data\n",
    "EClassTestPocket = ErrorRate(wPock,testData,testTarget)\n",
    "print(\"The Classification Error using the testing data is {:f} or {:f}% (Pocket)\".format(EClassTestPocket,EClassTestPocket*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6a7ed57aa02cf3caa6db301fda7f05c",
     "grade": false,
     "grade_id": "cell-a22b06b71e7333b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "State the test error results for the PLA and pocket algorithm that you obtained. Briefly discuss if they are as you expected and why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2045865b533306dadc6541971809cc72",
     "grade": true,
     "grade_id": "cell-ee0749d36e1f6ba7",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "The PLA and Pocket Algorithm were each trained using the training set for 100 iterations. The trained weights were then used to classify the test dataset. The classification errors using the test dataset are as indicated in te table below.\n",
    "\n",
    "| Algorithm | Error (Testing Set) |\n",
    "|:---------:|:-------------------:|\n",
    "|    PLA    |       0.027586      |\n",
    "|   Pocket  |       0.034483      |\n",
    "\n",
    "One might've expected the Pocket algorithm to yield a better result. However,\n",
    "\n",
    "1. Both the PLA and Pocket Algorithm (which internally calls PLA) make weight updates based on a random selection of a misclassified sample. If the pseudo random number generator is not re-seeded and the algorithms above are once again trained on the same dataset, the results will be different.\n",
    "\n",
    "\n",
    "2. Considering the random selection being made, perhaps running each algorithm for more number of iterations could have illustrated the difference in behavior better.\n",
    "\n",
    "Considering these facts, the results for this test are not all that surprising.\n",
    "\n",
    "That said, in case of data that is not linearly separable, PLA can keep varying between weight vectors and might not necessarily return the best. In such a scenario, running the pocket algorithm (over a large number of iterations) can at least yield the best set of weights that minimizes the classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a991507e56c1c7bb3a0f686df6e80515",
     "grade": false,
     "grade_id": "cell-e63e3e21f2a9ad08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The second classifier we consider is logistic regression.  Logistic regression computes the probability measure\n",
    "$$\\hat{y}=\\theta(\\underline{w}^T\\underline{x})$$\n",
    "for a feature vector $\\underline{x}$, where $\\theta(z)=\\mathrm{e}^s/(1+\\mathrm{e}^s)$ is the logistic function. Given $N$ data samples $\\underline{x}_n$ and labels $y_n$, the error measure for logistic regression is the binary cross-entropy loss\n",
    "$$L(\\underline{w})=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]\\;.$$\n",
    "For the following, you will consider the regularized loss function\n",
    "$$L_\\lambda(\\underline{w})=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]+\\frac{\\lambda}{2}\\|\\underline{w}\\|_2^2$$\n",
    "with the regularization parameter $\\lambda \\ge 0$. \n",
    "\n",
    "The training of the weight vector $\\underline{w}$ will be performed through batch gradient descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db5e733b839e11cc95526c870678e2a1",
     "grade": false,
     "grade_id": "cell-68358d532c780cc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Loss function [3 points]\n",
    "\n",
    "Implement a function to compute the regularized cross-entropy loss as defined above. The function has four input arguments: the weight vector, the feature vectors, the labels, and the regularization parameter. It returns the regularized loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "364ced88d6193816c529eb80c8df2b34",
     "grade": true,
     "grade_id": "cell-aebbe742cfbb66f8",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "# Regularized Cross Entropy Loss\n",
    "def crossEntropyLoss(w, x, y, reg):\n",
    "    theta = sigmoid(np.matmul(x,w))\n",
    "    loss = ((np.matmul((np.log(theta)).T,-1.0*y).item()-np.matmul((np.log(1-theta)).T,(-1.0*y)+1).item())/y.size)+(0.5*reg*np.ndarray.item(np.matmul(w.T,w)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e75d612d7fa35907b7240385d95df34f",
     "grade": false,
     "grade_id": "cell-8d0a18f2be2713da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Gradient [4 points]\n",
    "\n",
    "Provide an analytical expression for the gradient of the regularized cross-entropy loss with respect to the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4d7b1b6eb20f9afc7eff02678dc1ec9",
     "grade": true,
     "grade_id": "cell-bea8165ab7196506",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "**Note:** All usage of $\\log$ refer to the natural logarithm.\n",
    "\n",
    "Given the regularized cross-entropy loss function,\n",
    "\n",
    "$$L_\\lambda(\\underline{w})=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]+\\frac{\\lambda}{2}\\|\\underline{w}\\|_2^2$$\n",
    "\n",
    "Where \n",
    "\n",
    "$$\\hat{y}=\\theta(\\underline{w}^T\\underline{x})$$ \n",
    "\n",
    "where $\\theta$ represents the sigmoid function defined as, \n",
    "\n",
    "$$\\theta\\left(z\\right)=\\frac{\\mathrm{e}^z}{(1+\\mathrm{e}^z)}=\\frac{1}{(1+\\mathrm{e}^{-z})}$$\n",
    "\n",
    "The gradient of  the loss function is given by,\n",
    "\n",
    "$$\\nabla L_\\lambda(\\underline{w})=\\frac{\\partial L_\\lambda(\\underline{w})}{\\partial \\underline{w}}$$\n",
    "\n",
    "The gradient can be computed in steps with the chain rule and the following two results.\n",
    "\n",
    "$$\\frac{\\text{d}\\theta(\\log{z})}{\\text{d}z}=\\frac{1}{z}\\ \\ \\ --> Eq 1$$\n",
    "\n",
    "$$\\frac{\\text{d}\\theta(z)}{\\text{d}z}=\\theta(z)\\left(1-\\theta(z)\\right)\\ \\ \\ --> Eq 2$$\n",
    "\n",
    "Proceeding to derive the Gradient,\n",
    "\n",
    "$$\\nabla L_\\lambda(\\underline{w})=\\frac{\\partial L_\\lambda(\\underline{w})}{\\partial \\underline{w}}=\\frac{\\partial{\\left(\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\hat{y}(\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\hat{y}(\\underline{x}_n)\\right)\\right]+\\frac{\\lambda}{2}\\|\\underline{w}\\|_2^2\\right)}}{\\partial{\\underline{w}}}$$\n",
    "\n",
    "$$=\\frac{\\partial{\\left(\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\log\\left(\\theta(\\underline{w}^T\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\theta(\\underline{w}^T\\underline{x}_n)\\right)\\right]+\\frac{\\lambda}{2}\\|\\underline{w}\\|_2^2\\right)}}{\\partial{\\underline{w}}}$$\n",
    "\n",
    "$$=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[\\frac{\\partial{\\left(-y_n\\log\\left(\\theta(\\underline{w}^T\\underline{x}_n)\\right)-(1-y_n)\\log\\left(1-\\theta(\\underline{w}^T\\underline{x}_n)\\right)\\right)}}{\\partial{\\underline{w}}}\\right]+\\lambda\\underline{w}$$\n",
    "\n",
    "$$=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-y_n\\frac{\\partial{\\left(\\log\\left(\\theta(\\underline{w}^T\\underline{x}_n)\\right)\\right)}}{\\partial{\\underline{w}}}-(1-y_n)\\frac{\\partial{\\left(\\log\\left(1-\\theta(\\underline{w}^T\\underline{x}_n)\\right)\\right)}}{\\partial{\\underline{w}}}\\right]+\\lambda\\underline{w}$$\n",
    "\n",
    "Applying the derivative of the log function from **Eq 1** above.\n",
    "\n",
    "$$=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[\\frac{-y_{n}}{\\theta\\left(\\underline{w}^{T}\\underline{x}_n\\right)}\\frac{\\partial{\\left(\\theta\\left(\\underline{w}^{T}\\underline{x}_n\\right)\\right)}}{\\partial{\\underline{w}}}-\\frac{1-y_n}{1-\\theta(\\underline{w}^Tx_n)}\\frac{\\partial{\\left(\\theta\\left(\\underline{w}^{T}\\underline{x}_n\\right)\\right)}}{\\partial{\\underline{w}}}\\right]+\\lambda\\underline{w}$$\n",
    "\n",
    "$$=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[\\left(\\frac{-y_n}{\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)}-\\frac{1-y_n}{1-\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)}\\right)\\frac{\\partial{\\left(\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\right)}}{\\partial{\\underline{w}}}\\right]+\\lambda\\underline{w}$$\n",
    "\n",
    "After some simplification,\n",
    "\n",
    "$$=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-\\left(\\frac{y_n-\\theta\\left(\\underline{w}^T\\underline{x}\\right)}{\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\left(1-\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\right)}\\right)\\frac{\\partial{\\left(\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\right)}}{\\partial{\\underline{w}}}\\right]+\\lambda\\underline{w}$$\n",
    "\n",
    "Applying the derivative of the sigmoid function from **Eq 2** above.\n",
    "\n",
    "$$=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[-\\left(\\frac{y_n-\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)}{\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\left(1-\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\right)}\\right)\\left(\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\left(1-\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)\\right)\\right)\\frac{\\partial{\\left(\\underline{w}^T\\underline{x}_n\\right)}}{\\partial{\\underline{w}}}\\right]+\\lambda\\underline{w}$$\n",
    "\n",
    "$$=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[\\underline{x}_n\\left(\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)-y_n\\right)\\right]+\\lambda\\underline{w}$$\n",
    "\n",
    "Therefore, the analytical expression for the gradient of the regularized cross-entropy loss with respect to the weight vector is given by \n",
    "\n",
    "$$\\nabla L_\\lambda(\\underline{w})=\\frac{\\partial L_\\lambda(\\underline{w})}{\\partial \\underline{w}}=\\frac{1}{N}\\sum\\limits_{n=1}^N\\left[\\underline{x}_n\\left(\\theta\\left(\\underline{w}^T\\underline{x}_n\\right)-y_n\\right)\\right]+\\lambda\\underline{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20569096badf58c8909bc82a5df77f5b",
     "grade": false,
     "grade_id": "cell-8f0c5afca5e9d417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a function to compute the gradient. The function has four input arguments: the weight vector, the feature vectors, the labels, and the regularization parameter. It returns the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8488b92818298680af5f1328ae5f347",
     "grade": true,
     "grade_id": "cell-b39c8177911aa256",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradCE(w, x, y, reg):\n",
    "    theta = sigmoid(np.matmul(x,w))\n",
    "    theta = theta - y\n",
    "    grad = ((np.matmul(x.T,theta))/y.size)+(reg*w)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f73e7673edffccd37ae61138a708241",
     "grade": false,
     "grade_id": "cell-e62e6251dc4d5a69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Gradient Descent Implementation [5 points]\n",
    "Using the gradient and cross-entropy loss function above, implement the batch gradient descent algorithm. The function should accept seven arguments: the weight vector, the feature vectors, the labels, the learning rate, the number of epochs, the regularization parameter, and an error tolerance (set to $10^{-7}$ for the experiments). The error tolerance will be used to terminate the gradient descent early, if the difference (i.e., its 2-norm) between the old and updated weights after one iteration is below the error tolerance. The function should return the optimized weight vector and the learning error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b2882a29db4cb0850585ece871680d6",
     "grade": true,
     "grade_id": "cell-df585dbae385a61c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_descent(w, x, y, eta, iterations, reg, error_tol):\n",
    "    ceLossList = list()\n",
    "    # 1 - w is already initialized to 0\n",
    "    # 2 - Start Loop\n",
    "    while iterations>0:\n",
    "        # 3 - Compute Gradient\n",
    "        grad = gradCE(w,x,y,reg)\n",
    "        # 4 - Set Step Direction\n",
    "        stepDir = -1.0*(grad/(np.linalg.norm(grad,2)))\n",
    "        wOld = np.copy(w)\n",
    "        # 5 - Update Weights\n",
    "        w = w + (eta*stepDir)\n",
    "        # Calculate Loss\n",
    "        ceLossList.append(crossEntropyLoss(w,x,y,reg))\n",
    "        # Early Terminate - Tolerance Condition\n",
    "        if np.linalg.norm((wOld-w),2)<error_tol:\n",
    "            return w, ceLossList, iterations\n",
    "        iterations-=1\n",
    "    return w, ceLossList, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "210fb89fe9e58d3cc7ae4d8767f0a92f",
     "grade": false,
     "grade_id": "cell-d446b8d2bb035471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tuning the Learning Rate [6 Points]: \n",
    "Write a script that excutes logistic regression using the gradient descent function from above to classify the two classes in the notMNIST dataset, loaded with the  `loadData` function. \n",
    "\n",
    "Set the number of epochs to $5,000$ and use the regularization parameter $\\lambda=0$. \n",
    "\n",
    "For the learning rate, consider $\\eta=5\\cdot 10^{-3},\\,10^{-3},\\,10^{-4}$.\n",
    "\n",
    "Train the classifier on the training data (`trainData, trainTarget`).\n",
    "\n",
    "The script should plot the training loss as a function of the number of training epochs and output the test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ad70c3f83e48152234d667aaec5df51",
     "grade": true,
     "grade_id": "cell-776e56a74f0c1ff1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss computed for Testing Dataset at Different Eta\n",
      "\n",
      "For eta = 0.005, the Test Loss = 0.149644\n",
      "For eta = 0.001, the Test Loss = 0.089862\n",
      "For eta = 0.0001, the Test Loss = 0.146803\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "d2 = loadData()\n",
    "trainData2 = d2[0].reshape(3500,-1)\n",
    "trainData2 = np.append(np.ones((trainData2.shape[0],1)),trainData2,axis=1)\n",
    "\n",
    "validData2 = d2[1].reshape(100,-1)\n",
    "validData2 = np.append(np.ones((validData2.shape[0],1)),validData2,axis=1)\n",
    "\n",
    "testData2 = d2[2].reshape(145,-1)\n",
    "testData2 = np.append(np.ones((testData2.shape[0],1)),testData2,axis=1)\n",
    "\n",
    "trainTarget2 = d2[3].astype(int)\n",
    "validTarget2 = d2[4].astype(int)\n",
    "testTarget2 = d2[5].astype(int)\n",
    "\n",
    "# Run Batch Gradient Descent\n",
    "epochs = 5000\n",
    "regParam = 0 # Lambda\n",
    "learningRate1 = 0.005 # eta1\n",
    "learningRate2 = 0.001 # eta2\n",
    "learningRate3 = 0.0001 # eta3\n",
    "errTolerance = 0.0000001 # tolerance\n",
    "\n",
    "# For Learning Rate 1\n",
    "op1 = grad_descent(np.zeros((trainData2.shape[1],1)),trainData2,trainTarget2,learningRate1,epochs,regParam,errTolerance)\n",
    "\n",
    "# For Learning Rate 2\n",
    "op2 = grad_descent(np.zeros((trainData2.shape[1],1)),trainData2,trainTarget2,learningRate2,epochs,regParam,errTolerance)\n",
    "\n",
    "# For Learning Rate 3\n",
    "op3 = grad_descent(np.zeros((trainData2.shape[1],1)),trainData2,trainTarget2,learningRate3,epochs,regParam,errTolerance)\n",
    "\n",
    "# Plots and Outputs\n",
    "plt.plot(op1[1],\"-r\",label=\"eta = 0.005\")\n",
    "plt.plot(op2[1],\"-b\",label=\"eta = 0.001\")\n",
    "plt.plot(op3[1],\"-g\",label=\"eta = 0.0001\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Cross Entropy Loss vs Epochs (Training Dataset)\")\n",
    "plt.show()\n",
    "\n",
    "# Compute Loss With Test Data\n",
    "\n",
    "# With Eta = 0.005\n",
    "ceLossEta1 = crossEntropyLoss(op1[0],testData2,testTarget2,regParam)\n",
    "\n",
    "# With Eta = 0.001\n",
    "ceLossEta2 = crossEntropyLoss(op2[0],testData2,testTarget2,regParam)\n",
    "\n",
    "# With Eta = 0.0001\n",
    "ceLossEta3 = crossEntropyLoss(op3[0],testData2,testTarget2,regParam)\n",
    "\n",
    "print(\"Cross Entropy Loss computed for Testing Dataset at Different Eta\\n\")\n",
    "print(\"For eta = 0.005, the Test Loss = {:f}\".format(ceLossEta1))\n",
    "print(\"For eta = 0.001, the Test Loss = {:f}\".format(ceLossEta2))\n",
    "print(\"For eta = 0.0001, the Test Loss = {:f}\".format(ceLossEta3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6d913fa578e2f1de303a3500bc7bbb7",
     "grade": false,
     "grade_id": "cell-c6f7f54f2dff0f79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Briefly discuss the impact of $\\eta$ on the training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6378c65241ef1cda0ffdcfba2f93af52",
     "grade": true,
     "grade_id": "cell-6e645f7cd4e256f0",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "$\\eta$ represents the step size in the gradient descent algorithm. As can be seen from the graph above, a larger step size has been helpful in quickly getting closer to the minimum of the cross-entropy loss function. At then end of 5000 iterations, none of the three curves seem to have gotten to 0 loss yet. However, judging by the trend of the three curves, one can appreciate the fact that a lower value of $\\eta$ results in a slower descent thereby prolonging the training time. For this data set, it would be efficient to pick $\\eta=0.005$.\n",
    "\n",
    "In general though, having either a very high or very low value of $\\eta$ may not be helpful.\n",
    "\n",
    "1. **Very Small** $\\eta$ : As seen in the experiment in the prior section, a very low value of $\\eta$ can prolong training if the starting point (depends on how weights are initialized) is very far away from the minimum.\n",
    "\n",
    "\n",
    "2. **Very Large** $\\eta$ : This can prove detrimental when close to the minimum. That is, a large $\\eta$ can lead to the situation where one keeps overshooting and missing the minimum at every step.\n",
    "\n",
    "The best choice would then be to have a dynamic $\\eta$ which is large at the start but decays as the number of steps increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f98ad898635a4e20c0ba17d09b766a25",
     "grade": false,
     "grade_id": "cell-0db7ea466ea500bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generalization [3 points]:\n",
    "Fix the learning rate to $\\eta=0.005$, and consider values for the regularization parameter $\\lambda = 0.001,\\, 0.01,\\, 0.1$. Measure the validation and test losses and state them in your answer below. Comment on the effect of regularization on performance as well as the rationale behind tuning $\\eta$ using the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c70cce6d267ed7403506522ebda6b51",
     "grade": true,
     "grade_id": "cell-eac467078c174916",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "**Note:** There is a mistake in this question. The last sentence should be \"*Comment on the effect of regularization on performance as well as the rationale behind tuning  $\\lambda$ using the validation set.*\". As discussed on Piazza (https://piazza.com/class/k536rind5166p6?cid=48)\n",
    "\n",
    "For a fixed step size, $\\eta=0.005$. The validation and test losses have been tabulated at different values of the regularization parameter, $\\lambda$.\n",
    "\n",
    "|           |              |    Cross-Entropy  Loss      |          |\n",
    "|:---------:|:------------:|:--------------:|:--------:|\n",
    "| $\\lambda$ | Training Set | Validation Set | Test Set |\n",
    "|     0     |   0.003820   |    0.173047    | 0.150299 |\n",
    "|   0.001   |   0.033632   |    0.107379    | 0.099225 |\n",
    "|    0.01   |   0.060219   |    0.105983    | 0.100137 |\n",
    "|    0.1    |   0.113885   |    0.145799    | 0.135534 |\n",
    "\n",
    "**Effect of $\\lambda$ on Performance:**\n",
    "\n",
    "The regularization parameter, $\\lambda$ is used to control overfitting. \n",
    "\n",
    "As can be seen in the table above at $\\lambda=0$, the model has a very low training error but a very high test error which indicates overfitting. Likewise, a higher value ($\\lambda=0.1$) has resulted in hight training as well as test errors.\n",
    "\n",
    "The best choice of $\\lambda$ seems to be a very small non-zero value. In the case above, $0.001$ and $0.01$ seem to be good choices for $\\lambda$. [the differences in errors between $\\lambda=0.001$ and $\\lambda=0.01$ are $\\triangle E_{Valid}=0.0013$ and $\\triangle E_{Test}=0.000912$]\n",
    "\n",
    "**Need for a Validation Set:**\n",
    "\n",
    "The validation set is a portion of the original dataset that is kept aside just like the test set. The validation set is similar to the test set in that neither is used to train the model and both are used to estimate the out-of-sample error. They differ in the fact that the validation set is still used to tune hyper parameters (like $\\lambda$) that affect the model.\n",
    "\n",
    "The training process invloves selection of different hyperparameters. Since the validation set was not itself used to train the model, it can serve as an unbiased way to measure the effectiveness of different combinations of hyperparameters. That is, one can train the model independently using different combinations of hyperparameters (say different $\\lambda$) and then evaluate the performance of each combination using the validation set. This way, the best set of hyperparameters can be chosen.\n",
    "\n",
    "Finally, the test set can be used to evaluate the fully trained model (with appropriately chosen hyperparameters) to get a sense of how well the model generalizes. In addition, the test set can also serve as a basis of comparison for different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
